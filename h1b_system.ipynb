{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sci\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "from math import isnan\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tableau20 = [(31, 119, 180), (174, 199, 232), (255, 127, 14), (255, 187, 120),\n",
    "             (44, 160, 44), (152, 223, 138), (214, 39, 40), (255, 152, 150),\n",
    "             (148, 103, 189), (197, 176, 213), (140, 86, 75), (196, 156, 148),\n",
    "             (227, 119, 194), (247, 182, 210), (127, 127, 127), (199, 199, 199),\n",
    "             (188, 189, 34), (219, 219, 141), (23, 190, 207), (158, 218, 229)]\n",
    "for i in range(len(tableau20)):\n",
    "    r, g, b = tableau20[i]\n",
    "    tableau20[i] = (r / 255., g / 255., b / 255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class H1bInfo():\n",
    "\n",
    "\n",
    "    #load the data from path\n",
    "    def read_process_data(self,dataPath):\n",
    "        H1B = pd.read_csv(dataPath)\n",
    "        return H1B\n",
    "\n",
    "\n",
    "    #Analyze the H1Bs by the status of their visa applications and show the plot\n",
    "    def showCASE_STATUS(self,H1B):\n",
    "        statusCount = H1B['CASE_STATUS'].value_counts()\n",
    "        statusTypes = statusCount.index.copy(deep=True)\n",
    "        statusTypes = statusTypes.values\n",
    "        statusTypes[4] = 'Others'   # PENDING QUALITY AND COMPLIANCE REVIEW - UNASSIGNED , simplified to 'other'\n",
    "        statusTypes = statusTypes[0:5]\n",
    "\n",
    "        statusValues = statusCount.copy()\n",
    "        statusValues = statusValues.values\n",
    "        statusValues[4] = np.sum(statusValues[4:7])  # sum all of the others\n",
    "        statusValues = statusValues[0:5]\n",
    "\n",
    "\n",
    "        piefig = plt.pie(statusValues, autopct='%1.00f%%', shadow=False, startangle=90)\n",
    "        plt.legend(statusTypes, loc=\"best\", prop={'size':6})\n",
    "        plt.axis('equal')\n",
    "        plt.tight_layout()\n",
    "        plt.title('Case Status percentage')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    #K-Means Clustering to seperate the h1b location\n",
    "    def K_meanAnalyze(self,H1B):\n",
    "        #Get the states values\n",
    "        H1City,H1State = H1B['WORKSITE'].str.split(', ',1).str\n",
    "        #Reading Latitude and Longitude data (taking out null)\n",
    "        H1LatLong = H1B.loc[H1State != 'NA']\n",
    "        H1LatLong = H1LatLong[['lon','lat']]\n",
    "        H1LatLong = H1LatLong.dropna()\n",
    "        H1Long = H1LatLong['lon'].values\n",
    "        H1Lat = H1LatLong['lat'].values\n",
    "\n",
    "        #K-Means clustering to see where the applicants were\n",
    "        nClusters = 10\n",
    "        kmeans = KMeans(n_clusters=nClusters, random_state=0).fit(H1LatLong.values)\n",
    "        #Getting the cluster for each case\n",
    "        kl = kmeans.labels_\n",
    "        H1LatLong['Cluster'] = kl\n",
    "        H1LatLong['State'] = H1State\n",
    "        H1LatLong = H1LatLong.dropna()\n",
    "        return H1LatLong\n",
    "\n",
    "    #show the plot of the h1b WORKSITE after k-mean\n",
    "    #dense is the parameter to control distance between point, when gainning the info from the H1LatLong\n",
    "    def showWORKSITE(self,dense,H1LatLong):\n",
    "\n",
    "\n",
    "        #dense = 2000/1000 (usually )\n",
    "        pltIndex = range(0,2892144,dense)\n",
    "\n",
    "        #plot collor\n",
    "        colorSet = ['ro','go','bo','co','mo','ko','r*','g*','b*','c*']\n",
    "        CIndex = range(0,10)\n",
    "\n",
    "        for i in CIndex:\n",
    "            plt.plot(H1LatLong.iloc[pltIndex,0].loc[H1LatLong.iloc[pltIndex,2] == i], H1LatLong.iloc[pltIndex,1].loc[H1LatLong.iloc[pltIndex,2] == i], colorSet[i])\n",
    "\n",
    "        #axis label name\n",
    "        plt.xlabel('Longitude')\n",
    "        plt.ylabel('Latitude')\n",
    "        plt.legend(CIndex, loc=\"upper left\", prop={'size':12})\n",
    "        plt.title('H1b application distribution')\n",
    "        plt.show()\n",
    "\n",
    "    #Salary Data according to clusters\n",
    "    def salaryAnalyze(self,H1B,H1LatLong):\n",
    "\n",
    "        nClusters = 10\n",
    "        H1Salary = H1LatLong\n",
    "        H1Salary['Salary'] = H1B['PREVAILING_WAGE']\n",
    "        H1Salary['FT'] = H1B['FULL_TIME_POSITION']\n",
    "\n",
    "        salaryMin = np.zeros(nClusters)\n",
    "        salaryMax = np.zeros(nClusters)\n",
    "        salaryMean = np.zeros(nClusters)\n",
    "        salaryMedian = np.zeros(nClusters)\n",
    "        salaryStd = np.zeros(nClusters)\n",
    "\n",
    "        nClusters = 10\n",
    "        for k in range(0,nClusters):\n",
    "            #which cluster belongs to\n",
    "            salaryClustered = H1Salary.loc[H1Salary['Cluster'] == k]\n",
    "            #only consider full-time employment\n",
    "            salaryClustered = salaryClustered.loc[salaryClustered['FT'] == 'Y']\n",
    "            #take out extreme outliers\n",
    "            salaryClustered = salaryClustered.loc[salaryClustered['Salary'] <= 5000000]\n",
    "            # take out non-paid positions\n",
    "            salaryClustered = salaryClustered.loc[salaryClustered['Salary'] > 0]\n",
    "            #drop N/A\n",
    "            salaryClustered = salaryClustered.dropna()\n",
    "\n",
    "            #each cluster's description\n",
    "            salaryMin[k] = np.min(salaryClustered['Salary'].values)\n",
    "            salaryMax[k] = np.max(salaryClustered['Salary'].values)\n",
    "            salaryMean[k] = np.mean(salaryClustered['Salary'].values)\n",
    "            salaryMedian[k] = np.median(salaryClustered['Salary'].values)\n",
    "            salaryStd[k] = np.std(salaryClustered['Salary'].values)\n",
    "        return [salaryMin,salaryMax,salaryMean,salaryMedian,salaryStd]\n",
    "\n",
    "    #show the salary detail after k-mean process\n",
    "    def showSALARY_table(self,salaryMin,salaryMax,salaryMean,salaryMedian,salaryStd):\n",
    "         #build a DataFrame to show.\n",
    "        salaryStats = pd.DataFrame()\n",
    "        salaryStats['Mean'] = salaryMean.astype(int)\n",
    "        salaryStats['Median'] = salaryMedian.astype(int)\n",
    "        salaryStats['Std'] = salaryStd.astype(int)\n",
    "        salaryStats['Min'] = salaryMin.astype(int)\n",
    "        salaryStats['Max'] = salaryMax\n",
    "\n",
    "        print(salaryStats)\n",
    "\n",
    "    #Plotting and comparison to the median US salary (2015)\n",
    "\n",
    "    def showSALARY_plot(self,salaryMedian,salaryMean):\n",
    "        colorSet = ['r','g','b','c','m','k','r','g','b','c']\n",
    "        colorSetS = ['r*','g*','b*','c*','m*','k*','r*','g*','b*','c*']\n",
    "        CIndex = range(0,10)\n",
    "        for i in CIndex:\n",
    "            plt.bar(i,salaryMedian[i], color = colorSet[i])\n",
    "\n",
    "        for i in CIndex:\n",
    "            plt.plot(i,salaryMean[i], colorSet[i])\n",
    "\n",
    "        plt.plot(np.arange(-0.5,10.5,1),55775*np.ones(11), \"b--\")\n",
    "        plt.xlabel('Cluster number.')\n",
    "        plt.ylabel('Median Salary (USD)')\n",
    "        plt.title('Salary for every work part')\n",
    "        plt.show()\n",
    "\n",
    "    #show top 6 company who has apply to the h1b for employee\n",
    "    def showTOP6com_table(self,H1B):\n",
    "        comptable = H1B['EMPLOYER_NAME'].value_counts().sort_values(ascending=False).head(6)\n",
    "        comptable.plot(kind='barh')\n",
    "        plt.ylabel('H1B number')\n",
    "        plt.xlabel('company_name')\n",
    "        plt.title('Top6 company application number')\n",
    "\n",
    "        comptable2=H1B[H1B['EMPLOYER_NAME'].isin(comptable.index.values)]\n",
    "        comptable2 = comptable2.groupby(['EMPLOYER_NAME','YEAR']).size().unstack()\n",
    "        comptable2.plot(kind='barh')\n",
    "        plt.ylabel('H1B number')\n",
    "        plt.xlabel('company_name')\n",
    "        plt.legend(loc=\"upper right\", prop={'size':8})\n",
    "        plt.title('Company history application number')\n",
    "        plt.show()\n",
    "\n",
    "    #show the h1b number in everyear's change\n",
    "    def showYearTrend_plot(self,H1B):\n",
    "        yearTrend = H1B['YEAR'].value_counts().sort_values(ascending=True)\n",
    "        yearTrend.plot(kind = 'bar')\n",
    "        plt.title('Recent year for H1B application number')\n",
    "        plt.show()\n",
    "\n",
    "    #show top20 popular Jobtitle and top10 Worksites for H1-B Visa holders\n",
    "    def showJOBTITLE_plot(self,H1B):\n",
    "        H1B['JOB_TITLE'].value_counts().sort_values(ascending=False).head(20).plot(kind='barh',color=tableau20)\n",
    "        plt.title('Top10 jobs for h1b application')\n",
    "        plt.show()\n",
    "        H1B['WORKSITE'].value_counts().head(10).plot(kind='barh',color=tableau20)\n",
    "        plt.title('Top10 cities for h1b application')\n",
    "        plt.show()\n",
    "\n",
    "    #show top20 salary mean Jobtitle\n",
    "    def showAVGSalary_plot(self,H1B):\n",
    "        avgWagePerJob = H1B.groupby(['JOB_TITLE']).mean()['PREVAILING_WAGE'].nlargest(20).plot(kind=\"barh\",color=tableau20)\n",
    "        plt.title('Salary for Top20 jobs')\n",
    "        plt.show()\n",
    "\n",
    "    #show the difference between fulltime job and part time job.\n",
    "    def showFullvsPart_plot(self,H1B):\n",
    "        fullTime = H1B.FULL_TIME_POSITION.value_counts().plot(kind = 'bar',color=[(0.2,0.8,0.2),(0.8,0.2,0.2)])\n",
    "        plt.ylabel('H1B number')\n",
    "        plt.xlabel('JOb Type')\n",
    "        plt.legend(loc=\"upper right\", prop={'size':8})\n",
    "        plt.title('Full time job VS Part-time job')\n",
    "        plt.show()\n",
    "\n",
    "    # CASE_STATUS situation predict. using decision Tree\n",
    "    def predict_decision_tree(self,H1B,job_TitleName,WORKSITE,EMPLOYER_NAME,PREVAILING_WAGE,dense):\n",
    "\n",
    "        #top10JobList = H1B['JOB_TITLE'].value_counts().sort_values(ascending=False).head(20).index.values\n",
    "        top10JobDB = H1B[H1B['JOB_TITLE']==job_TitleName]\n",
    "        #remove 2016, use other years as training day\n",
    "        top10JobDB1 = top10JobDB[top10JobDB['YEAR'] != 2016]\n",
    "        sampler = np.random.permutation(top10JobDB1.count()/dense)\n",
    "        top10JobDB1 = top10JobDB1.take(sampler)\n",
    "\n",
    "\n",
    "        #-----------  data process  -----------\n",
    "        xSet1 = top10JobDB1.loc[:,'WORKSITE']\n",
    "        WORKSITE_List = xSet1.drop_duplicates().values\n",
    "        X_input1 = []\n",
    "        for i in xSet1:\n",
    "            X_input1.append(WORKSITE_List.tolist().index(i))\n",
    "\n",
    "        xSet2 = top10JobDB1.loc[:,'EMPLOYER_NAME']\n",
    "        EMPLOYER_NAME_List = xSet2.drop_duplicates().values\n",
    "        X_input2 = []\n",
    "        for i in xSet2:\n",
    "            X_input2.append(EMPLOYER_NAME_List.tolist().index(i))\n",
    "\n",
    "        xSet3 = top10JobDB1.loc[:,'CASE_STATUS']\n",
    "        CASE_STATUS_List = xSet3.drop_duplicates().values\n",
    "        X_input3 = []\n",
    "        for i in xSet3:\n",
    "            X_input3.append(CASE_STATUS_List.tolist().index(i))\n",
    "\n",
    "        #salary range\n",
    "        xSet4 = top10JobDB1.loc[:,'PREVAILING_WAGE']\n",
    "        #define salary range\n",
    "        salaryRange = [20000.00,40000.00,60000.00,80000.00,100000.00,120000.00,140000.00,\n",
    "                       160000.00,180000.00,200000.00,400000.00,500000.00,1000000.00,2000000.00,float(\"inf\")]\n",
    "        X_input4 = []\n",
    "        count = 0\n",
    "        for i in xSet4:\n",
    "            # deal with na !\n",
    "            if isnan(i):\n",
    "                i = 0\n",
    "            for j in range(0,len(salaryRange)):\n",
    "                if i <= salaryRange[j]:\n",
    "                    X_input4.append(j)\n",
    "                    break\n",
    "                elif i > salaryRange[j]:\n",
    "                    continue\n",
    "\n",
    "\n",
    "        # -----------  decision Tree ALG  -----------\n",
    "        x_input = np.zeros([3,len(X_input4)])\n",
    "        x_input[0] = X_input1\n",
    "        x_input[1] = X_input2\n",
    "        x_input[2] = X_input4\n",
    "        x_matrix = x_input.transpose()\n",
    "\n",
    "        clf = tree.DecisionTreeClassifier()\n",
    "        clf = clf.fit(x_matrix, X_input3)\n",
    "        #predict\n",
    "        salary_level = 0\n",
    "        for j in range(0,len(salaryRange)):\n",
    "                if PREVAILING_WAGE <= salaryRange[j]:\n",
    "                    salary_level = j\n",
    "                    break\n",
    "                elif i > salaryRange[j]:\n",
    "                    continue\n",
    "\n",
    "        if WORKSITE in WORKSITE_List.tolist():\n",
    "            worksite = WORKSITE_List.tolist().index(WORKSITE)\n",
    "        else:\n",
    "            worksite = 0\n",
    "        if EMPLOYER_NAME in EMPLOYER_NAME_List.tolist():\n",
    "            employ = EMPLOYER_NAME_List.tolist().index(EMPLOYER_NAME)\n",
    "        else:\n",
    "            employ = 0\n",
    "\n",
    "\n",
    "        tree_result = clf.predict([[worksite, employ,salary_level ]])\n",
    "\n",
    "        return CASE_STATUS_List[tree_result]\n",
    "\n",
    "\n",
    "    # test one specify job's accuracy in 2016 base on 2011-2015 data\n",
    "    def test_prediction_accuracy(self,H1B,job_TitleName,dense=10):\n",
    "        top10JobDB = H1B[H1B['JOB_TITLE']==job_TitleName]\n",
    "        # 2016data\n",
    "        top10JobDB1 = top10JobDB[top10JobDB['YEAR'] == 2016]\n",
    "        num_of_row = len(top10JobDB1.index)\n",
    "\n",
    "        accuracy_count = 0\n",
    "        for i in range(0,num_of_row,dense):\n",
    "            work_site = top10JobDB1[i:i+1]['WORKSITE'].values\n",
    "            emp_name = top10JobDB1[i:i+1]['EMPLOYER_NAME'].values\n",
    "            wage = top10JobDB1[i:i+1]['PREVAILING_WAGE'].values\n",
    "            if self.predict_decision_tree(top10JobDB,job_TitleName,work_site,emp_name,wage,dense) == top10JobDB1[i:i+1]['CASE_STATUS'].values:\n",
    "                accuracy_count +=1\n",
    "\n",
    "        #print('accuracy_count')\n",
    "        #print(accuracy_count)\n",
    "        if num_of_row == 0:\n",
    "\n",
    "            return 'no job data'\n",
    "        else:\n",
    "\n",
    "            #print(num_of_row)\n",
    "            return float(accuracy_count)/num_of_row*dense\n",
    "\n",
    "    #get top10 popular job's accuracy in 2016 base on 2011-2015\n",
    "    def testTOP10JOB_acuracy(self,H1B):\n",
    "        top10 = ['COMPUTER PROGRAMMER','SYSTEMS ANALYST','SOFTWARE DEVELOPER','BUSINESS ANALYST','COMPUTER SYSTEMS ANALYST',\n",
    "                'TECHNOLOGY LEAD - US','SENIOR SOFTWARE ENGINEER','TECHNOLOGY ANALYST - US','ASSISTANT PROFESSOR','SENIOR CONSULTANT']\n",
    "\n",
    "\n",
    "\n",
    "        dense = 10\n",
    "\n",
    "        accuracy_rate = []\n",
    "        for item in top10:\n",
    "            accuracy_rate.append(self.test_prediction_accuracy(H1B,item,dense))\n",
    "\n",
    "\n",
    "        result = dict(zip(top10,accuracy_rate))\n",
    "\n",
    "\n",
    "\n",
    "        print(result)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data file read successfully\n"
     ]
    }
   ],
   "source": [
    "h1bObj = H1bInfo()\n",
    "try:\n",
    "    H1B = h1bObj.read_process_data('h1b_kaggle.csv')\n",
    "except IOError:\n",
    "    print(\"Error: cannot find the path,pleas put data and program in the same folder\")\n",
    "else:\n",
    "    print(\"data file read successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k_meanResult = h1bObj.K_meanAnalyze(H1B)\n",
    "salary_Result = h1bObj.salaryAnalyze(H1B,k_meanResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h1bObj.showWORKSITE(H1B,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h1bObj.showCASE_STATUS(H1B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h1bObj.showTOP6com_table(H1B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h1bObj.showYearTrend_plot(H1B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h1bObj.showJOBTITLE_plot(H1B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h1bObj.showAVGSalary_plot(H1B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#=============================================\n",
    "#K-means and Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Mean  Median    Std    Min        Max\n",
      "0  72241   67579  35587   2012  4865100.0\n",
      "1  68792   64563  38669   2011  4648800.0\n",
      "2  67338   62171  34323  15080  4180800.0\n",
      "3  68359   63981  34810  14000  4996888.0\n",
      "4  65174   61734  28070  15226  4538560.0\n",
      "5  84536   80371  34519   2000  4985760.0\n",
      "6  63692   58157  48857  15080  4848792.0\n",
      "7  61082   55515  37206  16827  1040000.0\n",
      "8  80774   81286  27908  14851  2897400.0\n",
      "9  67026   63107  36651     63  4253600.0\n"
     ]
    }
   ],
   "source": [
    "h1bObj.showSALARY_table(salary_Result[0],salary_Result[1],salary_Result[2],salary_Result[3],salary_Result[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h1bObj.showSALARY_plot(salary_Result[3],salary_Result[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h1bObj.showWORKSITE(50,k_meanResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Case Status:\n",
      "['CERTIFIED']\n"
     ]
    }
   ],
   "source": [
    "job_title = 'SOFTWARE DEVELOPER'\n",
    "worksite = 'SAN JOSE, CALIFORNIA'\n",
    "emp_name = 'FACEBOOK'\n",
    "wage = 180000.0\n",
    "out = h1bObj.predict_decision_tree(H1B, job_title, worksite, emp_name, wage, dense=10)\n",
    "print('Predicted Case Status:')\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Case Status:\n",
      "['CERTIFIED-WITHDRAWN']\n"
     ]
    }
   ],
   "source": [
    "job_title = 'PROGRAMMER'\n",
    "worksite = 'BIRMINGHAM, ALABAMA'\n",
    "emp_name = 'WHATEVER INC'\n",
    "wage = 10000.0\n",
    "out = h1bObj.predict_decision_tree(H1B, job_title, worksite, emp_name, wage, dense=10)\n",
    "print('Predicted Case Status:')\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy rate:\n",
      "0.2857142857142857\n"
     ]
    }
   ],
   "source": [
    "job_title = 'PROGRAMMER'\n",
    "out = h1bObj.test_prediction_accuracy(H1B, job_title)\n",
    "print('Accuracy rate:')\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h1bObj.testTOP10JOB_acuracy(H1B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datetime import datetime\n",
    "\n",
    "def get_data(path_to_data, chunk='all'):\n",
    "    if chunk == 'all':\n",
    "        dwdata = pd.read_csv(path_to_data)\n",
    "    else:\n",
    "        dwdata = pd.read_csv(path_to_data, nrows = chunk)\n",
    "        \n",
    "    return dwdata\n",
    "\n",
    "def format_clean(data):\n",
    "    data2 = data.drop('Unnamed: 0', axis = 1)\n",
    "    df1 = data2[data2['CASE_STATUS']!='WITHDRAWN']\n",
    "    df2 =df1[df1['CASE_STATUS']!='CERTIFIED-WITHDRAWN']\n",
    "    \n",
    "    df3 = pd.get_dummies(df2.drop('CASE_STATUS', axis = 1))\n",
    "    df4 = pd.concat([df3, df2['CASE_STATUS']], axis=1)\n",
    "    df5 = df4.dropna()\n",
    "    \n",
    "    return df5\n",
    "\n",
    "\n",
    "data_set_path = 'h1b_kaggle.csv'\n",
    "\n",
    "chunks = 10000\n",
    "data_set = get_data(data_set_path, chunk = chunks)\n",
    "\n",
    "new_data = format_clean(data_set)\n",
    "\n",
    "xs = new_data.drop('CASE_STATUS', axis = 1)  \n",
    "ys = new_data['CASE_STATUS']\n",
    "X_train, X_test, y_train, y_test = train_test_split(xs,ys, test_size=0.30)\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "LR = LogisticRegression()\n",
    "LR.fit(scaler.transform(X_train), y_train)\n",
    "y_pred=LR.predict(scaler.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9428007889546351\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {}\".format(accuracy_score(y_test,y_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
